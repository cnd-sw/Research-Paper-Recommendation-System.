# -*- coding: utf-8 -*-
"""Research Paper Recommender Systems

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kx0FwUjhiapSkb3I2WeGi_GtHWtX7sFJ

Loading Data
"""

from google.colab import drive
drive.mount('/content/drive')

import seaborn as sns
import matplotlib.pyplot as plt
import os
import json
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.losses import cosine_similarity
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

import pandas as pd

# Define the file path
file_path = '/content/drive/MyDrive/Dataset/chandan.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Now you can work with the DataFrame 'df' in Colab

df[1:]

df.shape

any(df.isna().sum())

df.info()

"""#Exploratory Data Analysis"""

df.isna().sum()

column_data_types = df.dtypes
print(column_data_types)

summary_stats=df.describe()
summary_stats

df = df.loc[:, ~df.columns.duplicated()]
df.head(3)

correlation_matrix = df.corr()
correlation_matrix

print(df.columns)

sns.countplot(data=df, x='year')
plt.title("Papers Released across Years")
plt.show()

#abstract length
df['length'] = df['abstract'].str.len()
df.head(10)

sns.boxplot(data=df, y='length')
plt.title("Length of Abstracts")
plt.show()

#word count
def word_count(x):
    return len(x.split())

df['word_count'] = df['abstract'].apply(word_count)
df.head()

sns.boxplot(data=df, y='word_count')
plt.title("Word Count in Abstracts")
plt.show()

print(f"Mean of Word Count: {df['word_count'].mean():.2f}\nMedian of Word Count: {df['word_count'].median()}")

"""#Universal Sentence Encoder"""

# Tensorflow Hub URL for Universal Sentence Encoder
MODEL_URL = "https://tfhub.dev/google/universal-sentence-encoder/4"

# KerasLayer
sentence_encoder_layer = hub.KerasLayer(MODEL_URL,
                                        input_shape=[],
                                        dtype=tf.string,
                                        trainable=False,
                                        name="use")

abstracts = df["abstract"].to_list()

# Setup for embeddings computation
embeddings = []
batch_size = 3000
num_batches = len(abstracts) // batch_size

# Compute Embeddings in batches
for i in range(num_batches):
    batch_abstracts = abstracts[i*batch_size : (i+1)*batch_size]
    batch_embeddings = sentence_encoder_layer(batch_abstracts)
    embeddings.extend(batch_embeddings.numpy())

# Embeddings for remaining abstracts
remaining_abstracts = abstracts[num_batches*batch_size:]
if len(remaining_abstracts) > 0:
    remaining_embeddings = sentence_encoder_layer(remaining_abstracts)
    embeddings.extend(remaining_embeddings.numpy())

embeddings = np.array(embeddings)
y = df.index

nn = KNeighborsClassifier(n_neighbors=6)
nn.fit(embeddings,y)

"""Visualising results"""

for _ in range(5):
    idx = random.randint(i, len(y))
    sample = df["title"][idx]
    dist, index = nn.kneighbors(X=embeddings[idx,:].reshape(1,-1))
    print(f"Sample:\n{sample}\n")
    for i in range(1,6):
        print(f"Recommendation {i}:\n{df['title'][index[0][i]]}\n")
    print("===\n")



from sklearn.metrics import precision_score, recall_score, f1_score

# Evaluate the model
predictions = []
ground_truth = []

for idx in range(len(y)):
    _, indices = nn.kneighbors(X=embeddings[idx,:].reshape(1,-1))
    # The first neighbor is always itself, so exclude it
    recommended_papers = indices[0][1:6]

    predictions.extend(recommended_papers)
    ground_truth.extend([idx] * 5)  # Since each paper is being compared to 5 other papers

precision = precision_score(ground_truth, predictions, average='micro')
recall = recall_score(ground_truth, predictions, average='micro')
f1 = f1_score(ground_truth, predictions, average='micro')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

